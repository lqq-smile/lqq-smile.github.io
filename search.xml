<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title></title>
      <link href="/2018/06/11/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E7%AE%80%E5%8D%95%E8%A7%A3%E9%87%8A/"/>
      <url>/2018/06/11/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E7%AE%80%E5%8D%95%E8%A7%A3%E9%87%8A/</url>
      <content type="html"><![CDATA[<p>极大似然函数花了我好多时间理解。我觉得很有必要写一篇博文解释一下。虽然网上有关的博客很多，但大多数都写得过于复杂，我想解释得简单一些。</p><h2 id="举栗子是最好的方法"><a href="#举栗子是最好的方法" class="headerlink" title="举栗子是最好的方法"></a>举栗子是最好的方法</h2><p><a href="https://www.zhihu.com/question/54082000/answer/137868083" target="_blank" rel="noopener">如何理解似然函数? - 陈琳的回答 - 知乎</a></p><p>引用知乎上这位朋友的回答，再扩展一下。</p><p>我也假设有一个特殊的骰子，不知道有多少面。我们知道一般骰子都是6面，但我这里的骰子面数未知，是特制的。设骰子的面数为$x$。现在我们拿这个骰子抛了1000次，其中我们发现其中“1”的这一面出现了100次。当然按照直觉我们认为这个骰子最有可能是10个面，但是怎么用数学证明出来呢？如果我们经过大量的实验知道了某件事情的结果，想要反过来推测这件事情发生的概率，那么这个推测就称为极大似然估计。</p><p>设这个骰子有N面，所以出现“1”面的概率就是${1 \over N}$。根据联合概率计算公式，发生1000次抛骰子而有100次某个面向上这个结果的概率为：</p><script type="math/tex; mode=display">P=({1 \over N})^{100}  (1-{1\over N})^{900}</script><p>那么，怎么求出N呢？所谓最大似然，是指N最有可能是什么。在这个函数里面，最有可能是不是说对函数P求导，只要求出导数为0那个N值就可以了？当导数为0时我们可以求出P的极值点。即，我们要求P的导数$P’$为0的情况。</p><p>但是，上面那个式子的导数不好求啊，900次方了都。但是，数学家们很聪明，他们发现对幂数函数取对数不会改变函数的单调性。设$y=f(x)$，那么，$ln(f(x))$的单调性与$f(x)$的单调性一样。即：$f’(x)$与$(ln(f(x)))’$的极值点是一样的。</p><p>于是，我们对P取对数：</p><script type="math/tex; mode=display">ln(P)=ln(({1 \over N})^{100}  (1-{1\over N})^{900})=100ln{1\over N}+900ln(1-{1\over N})</script><p>对数公式就不解释啦，这个可以百度。对数真是一个神奇的东西，可以把这么复杂的幂函数化简成这个样子。补充数学公式：</p><blockquote><ol><li><p>ln(ab)=lna+lnb</p></li><li><p>$(lnx)’={1\over x}$</p></li><li><p>$(x^n)’=nx^{n-1}$</p></li><li><p>复合函数$f(g(x))$的导数为：</p><script type="math/tex; mode=display">f'(g(x))=g'(x)*f'(g(x))</script></li></ol></blockquote><p>所以：</p><script type="math/tex; mode=display">(ln(P))'=(100ln{1\over N}+900ln(1-{1\over N}))'=100*N*-1*{1\over N^2}+900*{N\over {N-1}}*{1\over N^2}</script><script type="math/tex; mode=display">(ln(P))'=900({1\over {N(N-1)}})-100{1\over N}</script><p>令$(ln(P))’=0$，得：</p><script type="math/tex; mode=display">900({1\over {N(N-1)}})=100{1\over N}</script><p>即：</p><script type="math/tex; mode=display">900{1\over{N-1}}=100</script><p>这是初中的方程哦，我们可以得出：</p><script type="math/tex; mode=display">N=10</script><p>所以，这个骰子最有可能有10个面。问题得解。</p>]]></content>
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 极大似然 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第一阶段总结与承诺</title>
      <link href="/2018/06/10/%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%93%E4%B8%8E%E6%89%BF%E8%AF%BA/"/>
      <url>/2018/06/10/%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%93%E4%B8%8E%E6%89%BF%E8%AF%BA/</url>
      <content type="html"><![CDATA[<h3 id="第一阶段总结"><a href="#第一阶段总结" class="headerlink" title="第一阶段总结"></a>第一阶段总结</h3><p>其实，我对我们个人敏捷的关注度不是很高。最近我忙于技术进修，对于个人敏捷的活动并不是非常关注，只是在交作业的时候做一下。还有一次班长要求我做录音讲解，那一次我是比较认真的。</p><p>我觉得我这样子主要是两方面的原因：</p><h4 id="我这次进修很重要，需要投入100-的精力"><a href="#我这次进修很重要，需要投入100-的精力" class="headerlink" title="我这次进修很重要，需要投入100%的精力"></a>我这次进修很重要，需要投入100%的精力</h4><p>我出来工作有好几年了，年龄大了以后就开始有各种想法，之前对于自己的了解不够，尝试过做管理，尝试过其他行业。后来终于发现自己其实还是喜欢技术，我是一个希望用技术改变世界的人。</p><p>我觉得我现在从事的技术工作对社会改变很有限，AI大热以后，我感觉人工智能可以改变世界，于是我现在一心投入到其中进行学习了。</p><p>人工智能需要的数学知识比较多，入门门槛很高，所以我真的有些顾不上我们的个人敏捷活动了。但是，我还是花时间留意大家的行动，从周老师的课程也获知到，自我管理是个人技能的重要维度，我不能退出个人敏捷的学习。只是暂时可能不能作为重点关注了。</p><h4 id="我之前认真研究过心理学，个人敏捷的理论跟我之前的学习有一定的重叠之处"><a href="#我之前认真研究过心理学，个人敏捷的理论跟我之前的学习有一定的重叠之处" class="headerlink" title="我之前认真研究过心理学，个人敏捷的理论跟我之前的学习有一定的重叠之处"></a>我之前认真研究过心理学，个人敏捷的理论跟我之前的学习有一定的重叠之处</h4><p>在加入个人敏捷之前，因为个人人际关系不和谐等各种原因，我曾经花了很多时间研究人际关系、心理学、情商等知识。加入个人敏捷群后，我发现个人敏捷的许多内容跟我之前研究的内容有重叠之处。所以，在这一点上说，我确实对我们的活动有一点放松。但是，我对个人敏捷是非常重视的。我会认真思考我们个人敏捷里面的每一个活动。</p><p>我们的第二阶段的内容主要是情绪管理。这也是我非常重视和乐于参与的内容。</p><p>之前，我的人际关系紧张，主要是自我情绪管理不好。一方面容易发脾气，另一方面说话时带情绪，让对方感到不舒服。</p><p>我们应该好好理解自己的情绪和别人的情绪。第一，我们只有正确理解了人性，才会感到幸福。第二，只有在情绪管理方面做好，我们的人际关系才和谐。第三，只有在情绪管理上面做好，才可以使自己能力提升，具备领导和管理团队的能力，也才可以有正确决策的能力。</p><p>情绪是一个长期的自我斗争的过程。三年前，我就开始接触心理学和人际交往相关课程了，但是，前几周我还因为情绪管理不好和领导争吵，差一点导致严重的后果。这件事也使我深深感觉到情绪管理的不易。</p><p>从心理学理论我们了解到，情绪是快于理智的。当我们感觉到威胁、不安等情境时，杏仁体会迅速发生作用，劫持我们的大脑，导致愤怒情绪的发生，于是下面就容易发生不愉快的事情。很多时候我们其实知道情绪是不对的，我们也许事后会后悔，但是，想在杏仁体发生作用前夺取大脑的控制权很难！</p><p>所以，我期望和大家一起进入情绪管理阶段的学习。如果我们不断和杏仁体劫持做斗争，相信终有一天我们在这方面会做得很好。</p><p>司马迁说过一句话：“夫勇者,卒然临之而不惊,无故加之而不怒.泰山崩于前而色不变,麋鹿行于左而目不瞬”。说的是做大事的人都有良好的情绪控制能力，遇到大事前，不会慌慌张张，不会被情绪左右自己，而是会冷静地处理。</p><h4 id="曾国藩是一个好榜样"><a href="#曾国藩是一个好榜样" class="headerlink" title="曾国藩是一个好榜样"></a>曾国藩是一个好榜样</h4><p>没有人一开始就可以做得很好。这几年，我们听到很多人都推崇一个人：曾国藩。他最大的成就就是在清朝对太平军束手无策的情况下，领导湘军屡败屡战，最终帮助清朝将太平天国镇压下去，官至极品。可是，曾国藩并不是一开始就是一个有大才的人。30岁之前，曾国藩其实是一个很平凡的人。他在自己的日记里面写道，他爱玩，喜欢炫耀学问，脾气大，还好色。并且多次在日记中痛斥自己的不当行为。即使曾国藩对自己要求很严格，他的朋友在他年轻的时候也从来没有觉得他有什么了不起，他的另一个同事胡林翼根本就看不起他，不想跟他来往。</p><p>创办湘军后，曾国藩也没有一开始就能做好。他曾经因为打败仗而要跳水自杀；曾经因为皇帝不重用而一气之下自己跑回老家丢下湘军不理了。</p><p>但是，曾国藩有一个很大的优点，他懂得反思，懂得改变。他每次受挫后一开始尽管也心情低落，但他总能吸取教训，并改正错误。一次一次碰壁后，他的情绪控制能力越来越好，脾气也越来越小了。有一句话叫做“宰相肚里能撑船”，后期的曾国藩也真的做到了。曾国藩有一个同事叫做左宗棠，这个人非常有才，但是脾气暴躁，而且也经常轻视曾国藩，认为曾国藩没有他厉害。他因为和曾国藩有过一些摩擦而毕生不太尊重曾国藩，经常在别的官员面前说他的坏话。然而曾国藩从来不放在心上。后来左宗棠要去打仗，缺粮食，曾国藩想都没想就给左筹到了，完全没有因为左宗棠的行为而在这件事上有所怠慢。在这件事可以看出，左宗棠在为人处世上不如曾国藩。</p><p>曾国藩的故事告诉我们，没有人一生下来就是圣人。无论多么优秀的人，都是在不断努力不断吸取教训之中走过来的，我们没有理由因为自己做得不够好而责怪自己。只要我们不断地改正自己的不良行为，总有一天也会像曾国藩一样成为大家尊重的人。</p><h3 id="承诺"><a href="#承诺" class="headerlink" title="承诺"></a>承诺</h3><p>最后我说说我对承诺的理解。</p><p>承诺是这周我们个人敏捷活动的主题。承诺的关键是我们要讨论如果我们对其他人做过某些承诺，我们做事情往往会更有责任心，做得更好。</p><p>我非常认可这个观点。我个人的经验也印证了这个观点。生活中，我是一个比较懒散的人，比较随意，宿舍往往乱得一塌糊涂。可是一旦答应了别人什么事情，从来不敢怠慢，主要是怕别人看不起自己，怕成为别人眼里那种不靠谱的人。</p><p>从这一点上看，其实我们可以利用人类的这个特点，请朋友监督自己。</p><p>如果我们想养成某一个好习惯，光靠自己努力往往事倍功半。因为我们对自己总是自律力不强的。但是如果我们承诺了朋友什么事情，效果往往会好很多。所以，我们为了向朋友有好的交代，一定会更加认真地做。上一次，我因为发脾气非常后悔，又怕自己以后轻易再犯，于是请一个朋友监督，承诺如果我以后又轻易发脾气，就请他吃三次饭。果然直到现在，我都做得挺好。</p><p><strong>重承诺，不要成为别人眼里不靠谱的人</strong></p><p>长辈总是教导我们，要讲信用。这句话是很有道理的。如果我们总是辜负别人的期望，别人还有什么理由相信我们呢？这样我们还能做什么大事呢？生活中我们眼里那些不成器的人往往就是这样形成的。</p><p>我今周的作业就讲到这里，希望能跟大家继续学习。</p>]]></content>
      
      <categories>
          
          <category> 个人敏捷 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 承诺 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PCA数学原理理解</title>
      <link href="/2018/06/10/PCA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E7%90%86%E8%A7%A3/"/>
      <url>/2018/06/10/PCA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E7%90%86%E8%A7%A3/</url>
      <content type="html"><![CDATA[<p>本文讨论对PCA数学原理的理解。</p><p>原文中最难理解的是：</p><ol><li>协方差矩阵的特征向量为什么是原始数据的降维方向？</li><li>协方差矩阵为什么可以并且要对角化？</li></ol><p>这两个问题其实是一个问题。</p><p>我们知道，协方差矩阵是一个对称矩阵，即：</p><script type="math/tex; mode=display">CC^T=C^TC</script><p>线性代数中，任何一个实对称矩阵，都有这样的特征：</p><blockquote><ol><li><p>它的特征向量一定正交。</p></li><li><p>特征向量单位化后组成的矩阵一定可以将原对称矩阵对角化。即：</p><script type="math/tex; mode=display">E^T C E=\Lambda</script></li></ol></blockquote><p>请理解我们要解决的根本问题：我们的目标是求得一个矩阵P，使Y=PX，最终结果是Y尽可能保留原数据的特征。那么，在数学上说，是不是我们要求得一个矩阵P，使得变换后的数据矩阵Y的协方差矩阵D不同维度的协方差为0？只要不同维度的协方差为0，那么不同的维度是不是相互独立，互不影响了呢？这样一来，重复的维度又不大重要的维度都被去掉了！</p><p>那么，上面的协方差为0，不就是一个对角矩阵$\Lambda$嘛！对角矩阵除了了对角线的元素之外，其他元素都是0. 正好满足了我们要求的目标。</p><p>上面我们知道，原始数据X虽然不对称，但它的协方差矩阵C是对称的，设C的特征向量矩阵为E，于是：</p><script type="math/tex; mode=display">E^TCE=\Lambda</script><p>我们还需要推导另一个关系,设D为Y的协方差矩阵，有：</p><script type="math/tex; mode=display">D={1 \over m}YY^T={1 \over m}(PX)(PX)^T={1 \over m}PXX^TP^T=P({1 \over m}XX^T)P^T=PCP^T</script><p>我们的目标是令D成为一个对角矩阵。于是：</p><script type="math/tex; mode=display">D=\Lambda=PCP^T=E^TCE</script><p>$E^T$就是我们最终要求的结果。即：$P=E^T$。</p><p>所以：</p><script type="math/tex; mode=display">Y=PX=E^TX</script><p>我们知道，X是已知的，于是我们可以求得对应的协方差矩阵C，通过C可以求特征向量矩阵$E^T$，从而求得对应的对角矩阵$\Lambda$。而事实上，$\Lambda$就是特征向量矩阵对应的对角为特征值的对角矩阵。我们按照特征值大小排序就是可以去掉特征值特别小的特征向量了。所以P是我们去掉一些特征后的最终结果。</p>]]></content>
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>对上不对下，乃是祸害之源</title>
      <link href="/2018/06/10/%E5%AF%B9%E4%B8%8A%E4%B8%8D%E5%AF%B9%E4%B8%8B%EF%BC%8C%E4%B9%83%E6%98%AF%E7%A5%B8%E5%AE%B3%E4%B9%8B%E6%BA%90/"/>
      <url>/2018/06/10/%E5%AF%B9%E4%B8%8A%E4%B8%8D%E5%AF%B9%E4%B8%8B%EF%BC%8C%E4%B9%83%E6%98%AF%E7%A5%B8%E5%AE%B3%E4%B9%8B%E6%BA%90/</url>
      <content type="html"><![CDATA[<h2 id="知乎上的话题"><a href="#知乎上的话题" class="headerlink" title="知乎上的话题"></a>知乎上的话题</h2><blockquote><p> <a href="https://zhuanlan.zhihu.com/p/31956198" target="_blank" rel="noopener">政策为何会变样：治理国家的艰难之处</a></p><p><a href="https://www.zhihu.com/question/26984147/answer/269660715" target="_blank" rel="noopener">为什么历史上有名的重臣，他们学的四书五经，读的是圣贤书，都没有学数理化，照样把国家治理的很好？</a></p><p><a href="https://www.zhihu.com/question/20422670/answer/15096800" target="_blank" rel="noopener">为什么宦官「弄权」就一定会「败坏朝纲」</a></p></blockquote><p>我对第三个问题比较感兴趣，先讨论这个问题吧。</p><h2 id="讨论问题"><a href="#讨论问题" class="headerlink" title="讨论问题"></a>讨论问题</h2><h3 id="为什么宦官「弄权」就一定会「败坏朝纲」"><a href="#为什么宦官「弄权」就一定会「败坏朝纲」" class="headerlink" title="为什么宦官「弄权」就一定会「败坏朝纲」"></a>为什么宦官「弄权」就一定会「败坏朝纲」</h3><p>要讨论问题，应该先释义，再分析，最后解决。</p><h4 id="释义"><a href="#释义" class="headerlink" title="释义"></a>释义</h4><p>什么叫做“朝纲”？如果不先解释这个词语，我们讨论起来就容易乱。朝纲按照百度百科的解释，就是朝廷的法纪。因此，这个问题实质是“为什么太监专权就一定会破坏朝廷的法纪”。</p><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>社会上人的大部分行为其实都可以解释为一个目的：<strong>趋利</strong>。</p><h5 id="太监为什么会专权"><a href="#太监为什么会专权" class="headerlink" title="太监为什么会专权"></a>太监为什么会专权</h5><p>太监专权，那么我们先分析一下太监为什么要专权。一个器官不全的人，放着正职不做，为什么要来干预皇家的事情？</p><p>原本太监只是皇宫里面干杂活的。皇帝身边除了妃子、侍卫之外，总要大量的人手服侍。其实服侍这种全部要女人也可以的，但无奈女人在生理上干不了重活。有一些事情非要男性才干得了。但是皇宫里面那种杂活通常要杂工一直住在里面，而且皇帝妃子们喜欢上一个工人了往往也一直留在身边。可这样一来，皇帝就容易戴绿帽子了。理论上，皇宫里面任何一个女人皇帝都可以要，也就是皇宫里面任何一个女人都可以看成皇帝的老婆，而皇帝又不能顾全到所有女人。要是这些女人身边有男人，不是太危险了吗？所以，这些干杂活的男人必须都得切了。</p><blockquote><p>题外话：为什么侍卫不用切？</p><p>虽然侍卫也在皇宫，但是他们有着严格的管理制度。</p><ol><li>侍卫通常不能进入后宫。侍卫一般只是守卫皇宫外围和皇帝身边，不允许进入妃子住的地方。</li><li>侍卫不能单独在皇宫里面乱逛。你在上班哦，以为逛菜市场呀。</li><li>侍卫实行轮班制。侍卫不会永远守在一个地方，通常一时守这里，一时守那里。没有与妃子们私通的机会。</li><li>侍卫通常是一群人一起巡逻，难道你要当着众人的面与妃子们交流？</li><li>即使皇帝下令抓捕妃子，也是太监动手，侍卫碰不到妃子的！</li></ol></blockquote><p>现在说说皇帝身边的太监。</p><p>皇帝跟普通的老板一样，一旦某一个太监使得顺手了，也就懒得换了。于是这个太监就会成为皇帝身边的左右手。</p><p><strong>皇帝在朝廷上并非可以为所欲为。</strong></p><p>如果读者对明朝的历史有所了解，可以发现，明朝的很多皇帝在大臣们面前并不是那么强势，有时候，皇帝居然被大臣们压得像猫一样！不会吧？怎么会这样！皇帝虽然有生杀大权，但是，他不能当疯子，不能说杀谁就杀谁的。他要行使权力总得令人心服口服。于是问题就来了，在嘴巴上，他不一定斗得过下面这群当了几十年官的老狐狸们！如明朝的一些皇帝，想下令做什么事情，下面的大臣就会跑出来说不行不行呀，这违反了祖宗定下来的规矩，这样上天会不高兴的！这时候皇帝必须有足够的理由说服大臣们，你不能总是一句“拉出去砍了”完事。然而，皇帝的政治经验比起大臣差得太远了！他是那么的无力！</p><p>到底是你们做皇帝还是我做皇帝！</p><p><strong>有时候，太监成为了皇帝最信任的政治助手</strong></p><p>一开始，太监只是皇帝身边做杂活的。但是时间一长，有些太监就升级了，他们成功获得皇帝的信任，成了左右手。皇帝一刻也离不开他们。再后来，皇帝和太监的关系又升级了。</p><p>前面已经说过，皇帝在朝廷不一定可以呼风唤雨，他也有很多烦恼。有时候，他会忍不住向身边的太监们倾诉。这时候，身边那些高情商的太监们就有了知遇之恩了，他们有了成为皇帝心腹的机会！</p><p>这些太监们当然会奉迎皇帝。他们会帮皇帝们出主意啊什么的，后来，皇帝干脆派他们办事了。于是，他们开始获得权力了。</p><p>他们办事是皇帝委任的，谁敢挡啊。你敢跟皇帝说这不符合朝廷的法制？中国一向是人情大于天，你事事按规矩办事，得罪的人可不知有多少。</p><p>皇帝为了制衡大臣，就开始倚重太监了。太监要办事，总要权力吧？慢慢，太监的权力越来越大了。</p><p><strong>没有人是傻子，太监也不是</strong></p><p>前面已经说过了，人都是趋利的。太监不会例外。</p><p>太监获得权力后，大家都会容让他，大家都要给他面子，这时候他怎么办呢？当然是想方设法保住自己的地位啦。<strong>他的一切行为无非是为自己着想。</strong></p><ol><li>他知道他不能得罪皇帝。</li><li>他知道换一个皇帝他不一定继续受宠。</li><li>他知道出来混，早晚都要还的。唯一不用还的办法是继续受宠。</li><li>他知道为了保住自己的地位，不择手段是必要的。对敌人仁慈就是对自己残忍。</li><li>他知道他的行为一开始就没有合法性。（没有哪一个朝代公开允许太监有权）</li></ol><p>为了证明自己不是傻子，太监只能混乱朝纲。</p><ol><li>失宠会生不如死。于是宁愿没有羞耻心，宁愿坏事做尽，就是不能失宠。</li><li>皇帝年纪小一点好控制。于是有太监专权者喜欢立幼，然后继续做坏事。</li><li>皇帝太有能力自己就危险了。于是有太监专权者想办法阻止有能力的人当皇帝，宁愿扶持没有能力的人做皇帝。</li><li>掌握军队指挥权更保险。他们得到军权后又会进一步危害朝廷。</li><li>他们知道所有人都看他们不顺眼，但他们不在乎。</li></ol><p>上面分析已经充分论证了太监专权的前因后果。</p><h3 id="政策为何会变样：治理国家的艰难之处"><a href="#政策为何会变样：治理国家的艰难之处" class="headerlink" title="政策为何会变样：治理国家的艰难之处"></a><a href="https://zhuanlan.zhihu.com/p/31956198" target="_blank" rel="noopener">政策为何会变样：治理国家的艰难之处</a></h3><p><strong>对上不对下，怎么可能不变样</strong></p><p>你讨好的对象，到底是老百姓还是领导？</p><p>真正决定你前途的是你的领导，不是老百姓。领导也一样。这样的体制，政策怎么可能不变样呢？上面已经论证过，人都是趋利的。人几乎一切行为本质上都不过是为了自己。</p><p>决定你命运的是老百姓，你就会讨好老百姓；决定你命运的只是你的领导，你就只会讨好你领导。</p><p>这样的艰难是出在根本制度上，没有什么解决方案的。</p><h3 id="为什么历史上有名的重臣，他们学的四书五经，读的是圣贤书，都没有学数理化，照样把国家治理的很好？"><a href="#为什么历史上有名的重臣，他们学的四书五经，读的是圣贤书，都没有学数理化，照样把国家治理的很好？" class="headerlink" title="为什么历史上有名的重臣，他们学的四书五经，读的是圣贤书，都没有学数理化，照样把国家治理的很好？"></a>为什么历史上有名的重臣，他们学的四书五经，读的是圣贤书，都没有学数理化，照样把国家治理的很好？</h3><p>这个问题本身论述有误。</p><p>怎么知道“照样把国家治理得很好”呢？中国历史那么长，真正社会欣欣向荣的时间很短，大部分时间都是一团糟。事实上，中国根本就没有产生过科学的政治理论。四书五经，只不过是在说道理。这些书籍，只能开启官员们的心智而已，根本不能使大家形成科学的共识。中国古代的官员讨论国家政策时，都只是各执己见，公说公有理，婆说婆有理，最后让皇帝拍板而已。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>没有监督的权力一定会成为祸害的根源；</p><p>权力导致腐败，绝对的权力导致绝对的腐败。</p>]]></content>
      
      <categories>
          
          <category> 政治历史 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 政策 </tag>
            
            <tag> 社会 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PCA的数学原理(转)</title>
      <link href="/2018/05/09/PCA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"/>
      <url>/2018/05/09/PCA%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/</url>
      <content type="html"><![CDATA[<p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。</p><p>当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白PCA的工作原理。</p><h2 id="1-数据的向量表示及降维问题"><a href="#1-数据的向量表示及降维问题" class="headerlink" title="1. 数据的向量表示及降维问题"></a>1. 数据的向量表示及降维问题</h2><p>一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下：</p><p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)</p><p>其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子：</p><script type="math/tex; mode=display">(500,240,25,13,2312.15)^T</script><p>注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。</p><p>我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p><p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p><p>举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。</p><p>当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。</p><p>这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p><p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p><p>要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。</p><h2 id="2-向量的表示及基变换"><a href="#2-向量的表示及基变换" class="headerlink" title="2. 向量的表示及基变换"></a>2. 向量的表示及基变换</h2><p>既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。</p><h2 id="3-内积与投影"><a href="#3-内积与投影" class="headerlink" title="3. 内积与投影"></a>3. 内积与投影</h2><p>下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：</p><script type="math/tex; mode=display">(a_1,a_2,...,a_n)^T  \cdot (b_1,b_2,...,b_n)^T=a_1b_1+a_2b_2+...+a_nb_n</script><p>内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设<img src="https://www.zhihu.com/equation?tex=A" alt="A">和<img src="https://www.zhihu.com/equation?tex=B" alt="B">是两个<img src="https://www.zhihu.com/equation?tex=n" alt="n">维向量，我们知道<img src="https://www.zhihu.com/equation?tex=n" alt="n">维向量可以等价表示为<img src="https://www.zhihu.com/equation?tex=n" alt="n">维空间中的一条从原点发射的有向线段，为了简单起见我们假设<img src="https://www.zhihu.com/equation?tex=A" alt="A">和<img src="https://www.zhihu.com/equation?tex=B" alt="B">均为二维向量，则$ A=(x_1,y_1),B=(x_2,y_2) $。则在二维平面上<img src="https://www.zhihu.com/equation?tex=A" alt="A">和<img src="https://www.zhihu.com/equation?tex=B" alt="B">可以用两条发自原点的有向线段表示，见下图：</p><p><img src="https://pic4.zhimg.com/80/8d64151ceed0eed4d4708d8d9e6374dc_hd.jpg" alt="图片"></p><p>好，现在我们从<img src="https://www.zhihu.com/equation?tex=A" alt="A">点向<img src="https://www.zhihu.com/equation?tex=B" alt="B">所在直线引一条垂线。我们知道垂线与<img src="https://www.zhihu.com/equation?tex=B" alt="B">的交点叫做<img src="https://www.zhihu.com/equation?tex=A" alt="A">在<img src="https://www.zhihu.com/equation?tex=B" alt="B">上的投影，再设<img src="https://www.zhihu.com/equation?tex=A" alt="A">与<img src="https://www.zhihu.com/equation?tex=B" alt="B">的夹角是$\alpha$，则投影的矢量长度为$\vert A \vert cos(\alpha)$,其中$\vert A \vert=\sqrt{x_1^2+y_1^2}$是向量<img src="https://www.zhihu.com/equation?tex=A" alt="A">的模，也就是<img src="https://www.zhihu.com/equation?tex=A" alt="A">线段的标量长度。</p><p>注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。</p><p>到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：</p><script type="math/tex; mode=display">A \cdot B=\vert A\vert  \vert B \vert cos(\alpha)</script><p>现在事情似乎是有点眉目了：<img src="https://www.zhihu.com/equation?tex=A" alt="A">与<img src="https://www.zhihu.com/equation?tex=B" alt="B">的内积等于<img src="https://www.zhihu.com/equation?tex=A" alt="A">到<img src="https://www.zhihu.com/equation?tex=B" alt="B">的投影长度乘以<img src="https://www.zhihu.com/equation?tex=B" alt="B">的模。再进一步，如果我们假设<img src="https://www.zhihu.com/equation?tex=B" alt="B">的模为1，即让$\vert B \vert = 1$，那么就变成了：</p><script type="math/tex; mode=display">A \cdot B = \vert A \vert cos(\alpha)</script><p>也就是说，设向量<img src="https://www.zhihu.com/equation?tex=B" alt="B">的模为1，则<img src="https://www.zhihu.com/equation?tex=A" alt="A">与<img src="https://www.zhihu.com/equation?tex=B" alt="B">的内积值等于<img src="https://www.zhihu.com/equation?tex=A" alt="A">向<img src="https://www.zhihu.com/equation?tex=B" alt="B">所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。</p><h2 id="4-基"><a href="#4-基" class="headerlink" title="4. 基"></a>4. 基</h2><p>下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：</p><p><img src="https://pic2.zhimg.com/df6a713c1b97cc55bd20afce46ace718_r.jpg" alt=""></p><p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为$(3,2)$，这是我们再熟悉不过的向量表示。</p><p>不过我们常常忽略，只有一个$(3,2)$本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量$(3,2)$实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。</p><p>更正式的说，向量(x,y)实际上表示线性组合：</p><script type="math/tex; mode=display">x(1,0)^T+y(0,1)^T</script><p>不难证明所有二维向量都可以表示为这样的线性组合。此处$(1,0)$和$(0,1)$叫做二维空间中的一组基。</p><p><img src="https://pic3.zhimg.com/4533331b5b4b5ea98c90f2abed81d470_r.jpg" alt=""><strong>所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。</strong>只不过我们经常省略第一步，而默认以$(1,0)$和$(0,1)$为基。</p><p>我们之所以默认选择$(1,0)$和$(0,1)$为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。</p><p>例如，$(1,1)$和$(-1,1)$也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为$({1 \over \sqrt{2}},{1 \over \sqrt{2}})$和$(-{1 \over \sqrt{2}},{1 \over \sqrt{2}})$ 。</p><p>现在，我们想获得 $(3,2)$在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算$(3,2)$和两个基的内积，不难得到新的坐标为$({5 \over \sqrt{2}},-{1 \over \sqrt{2}})$。下图给出了新的基以及$(3,2)$在新基上坐标值的示意图：</p><p><img src="https://pic4.zhimg.com/ff47d66fa67d12918e4e83678fa6b78d_r.jpg" alt="">另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。</p><h2 id="5-基变换的矩阵表示"><a href="#5-基变换的矩阵表示" class="headerlink" title="5. 基变换的矩阵表示"></a>5. 基变换的矩阵表示</h2><p>下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将$(3,2)$变换为新基上的坐标，就是用$(3,2)$与第一个基做内积运算，作为第一个新的坐标分量，然后用$(3,2)$与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：</p><script type="math/tex; mode=display">\begin{bmatrix} {1 \over \sqrt{2}}&{1 \over \sqrt{2}} \\ -{1 \over \sqrt{2}}&{1 \over \sqrt{2}}  \end{bmatrix} \begin{bmatrix} 3\\2 \end{bmatrix} = \begin{bmatrix} {5 \over \sqrt{2}} \\ -{1 \over \sqrt{2}} \end{bmatrix}</script><p>太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如$(1,1),(2,2),(3,3)$想变换到刚才那组基上，则可以这样表示：</p><script type="math/tex; mode=display">\begin{bmatrix} {1 \over \sqrt{2}}&{1 \over \sqrt{2}} \\ -{1 \over \sqrt{2}}&{1 \over \sqrt{2}}  \end{bmatrix} \begin{bmatrix} 1&2&3\\1&2&3 \end{bmatrix} = \begin{bmatrix} {2 \over \sqrt{2}} & {4 \over \sqrt{2}}& {6 \over \sqrt{2}} \\ 0&0&0 \end{bmatrix}</script><p>于是一组向量的基变换被干净的表示为矩阵的相乘。</p><p>一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。</p><p>数学表示为：</p><script type="math/tex; mode=display">\begin {pmatrix}  p_1\\p_2\\ \vdots \\ p_R  \end{pmatrix} \begin {pmatrix} a_1&a_2&\cdots &a_M \end{pmatrix}=\begin {pmatrix} p_1a_1&p_1a_2&\cdots &p_1a_M \\ p_2a_1&p_2a_2&\cdots &p_2a_M \\ \vdots & \vdots & \ddots & \vdots \\   p_Ra_1&p_Ra_2&\cdots &p_Ra_M   \end{pmatrix}</script><p>其中$p_i$是一个行向量，表示第$i$个基，$a_j$是一个列向量，表示第$j$个原始数据记录。</p><p>特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。</p><p>最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。</p><h2 id="6-协方差矩阵及优化目标"><a href="#6-协方差矩阵及优化目标" class="headerlink" title="6. 协方差矩阵及优化目标"></a>6. 协方差矩阵及优化目标</h2><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？</p><p>要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。</p><p>为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：</p><script type="math/tex; mode=display">\begin{pmatrix} 1&1&2&4&2 \\ 1&3&3&4&4  \end{pmatrix}</script><p>其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。</p><p>我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后：</p><script type="math/tex; mode=display">\begin{pmatrix} -1&-1&0&2&0 \\ -2&0&0&1&1 \end{pmatrix}</script><p>我们可以看下五条数据在平面直角坐标系内的样子：</p><p><img src="https://pic2.zhimg.com/e01296f282109b59e18086843866f81a_r.jpg" alt=""></p><p>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？</p><p>通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。</p><p>那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p><p>以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。</p><p>下面，我们用数学方法表述这个问题。</p><h2 id="7-方差"><a href="#7-方差" class="headerlink" title="7. 方差"></a>7. 方差</h2><p>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：</p><script type="math/tex; mode=display">Var(a)={1 \over m} \sum_{i=1}^m(a_i-\mu)^2</script><p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p><script type="math/tex; mode=display">Var(a)={1 \over m} \sum_{i=1}^ma_i^2</script><p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p><h2 id="8-协方差"><a href="#8-协方差" class="headerlink" title="8. 协方差"></a>8. 协方差</h2><p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p><p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p><p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p><script type="math/tex; mode=display">Cov(x,y)={1 \over m}\sum_{i=1}^ma_ib_i</script><p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p><p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p><p>至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。</p><h2 id="9-协方差矩阵"><a href="#9-协方差矩阵" class="headerlink" title="9. 协方差矩阵"></a>9. 协方差矩阵</h2><p>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p><p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：</p><p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：</p><script type="math/tex; mode=display">X=\begin{pmatrix} a_1&a_2&...&a_m \\b_1&b_2&...&b_m  \end{pmatrix}</script><p>然后我们用X乘以X的转置，并乘上系数1/m：</p><script type="math/tex; mode=display">{1 \over m}XX^T=\begin{pmatrix} {1\over m } \sum_{i=1}^m a_i^2 &  {1\over m } \sum_{i=1}^m a_ib_i \\ {1\over m } \sum_{i=1}^m a_ib_i  & {1\over m } \sum_{i=1}^m b_i^2  \end{pmatrix}</script><p>奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p><p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：</p><p>设我们有<img src="https://www.zhihu.com/equation?tex=m" alt="m">个<img src="https://www.zhihu.com/equation?tex=n" alt="n">维数据记录，将其按列排成<img src="https://www.zhihu.com/equation?tex=n" alt="n">乘<img src="https://www.zhihu.com/equation?tex=m" alt="m">的矩阵<img src="https://www.zhihu.com/equation?tex=+X" alt=" X">，设$C={1 \over m }XX^T$</p><p>，则<img src="https://www.zhihu.com/equation?tex=C" alt="C">是一个对称矩阵，其对角线分别个各个字段的方差，而第$i$行$j$列和$j$行$i$列元素相同，表示$i$和两个字段的$j$协方差。</p><h2 id="10-协方差矩阵对角化"><a href="#10-协方差矩阵对角化" class="headerlink" title="10. 协方差矩阵对角化"></a>10. 协方差矩阵对角化</h2><p>根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p><p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系:</p><script type="math/tex; mode=display">D={1 \over m} YY^T   ={1 \over m} (PX)(PX)^T    ={1 \over m} PXX^TP^T    =P({1 \over m}XX^T)P^T    = PCP^T</script><p>现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p>至此，我们离“发明”PCA还有仅一步之遥！</p><p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。</p><p>由上文知道，协方差矩阵<img src="https://www.zhihu.com/equation?tex=C" alt="C">是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p><ol><li>实对称矩阵不同特征值对应的特征向量必然正交。</li><li>设特征向量$\lambda$重数为r，则必然存在r个线性无关的特征向量对应于$\lambda$，因此可以将这r个特征向量单位正交化。</li></ol><p>由上面两条可知，一个<img src="https://www.zhihu.com/equation?tex=n" alt="n">行<img src="https://www.zhihu.com/equation?tex=n" alt="n">列的实对称矩阵一定可以找到n个单位正交特征向量，设这<img src="https://www.zhihu.com/equation?tex=n" alt="n">个特征向量为$e_1,e_2,…,e_n$，我们将其按列组成矩阵：</p><script type="math/tex; mode=display">E=\begin{pmatrix} e_1&e_2 &...&e_n\end{pmatrix}</script><p>则对协方差矩阵C有如下结论：</p><script type="math/tex; mode=display">E^TCE=\Lambda=\begin{pmatrix} \lambda_1&&&\\& \lambda_2&&\\&&\ddots&\\&&&\lambda_n \end{pmatrix}</script><p>其中$\Lambda$为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p><p> 以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p><p>到这里，我们发现我们已经找到了需要的矩阵P：</p><script type="math/tex; mode=display">P=E^T</script><p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照$\Lambda$中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。</p><h2 id="11-算法及实例"><a href="#11-算法及实例" class="headerlink" title="11. 算法及实例"></a>11. 算法及实例</h2><p>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p><p><strong>PCA算法</strong></p><p>总结一下PCA的算法步骤：</p><p>设有m条n维数据。</p><ol><li>将原始数据按列组成n行m列矩阵X</li><li>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li><li>求出协方差矩阵$C={1\over m}XX^T$</li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</li><li>Y=PX即为降维到k维后的数据</li></ol><p><strong>实例1</strong></p><p>这里以上文提到的</p><script type="math/tex; mode=display">\begin{pmatrix} -1&-1&0&2&0 \\ -2&0&0&1&1\end{pmatrix}</script><p>为例，我们用PCA方法将这组二维数据其降到一维。</p><p>因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：</p><script type="math/tex; mode=display">C={1 \over 5}\begin{pmatrix}-1&-1&0&2&0 \\ -2&0&0&1&1 \end{pmatrix}\begin{pmatrix}-1&-2\\-1&0\\0&0\\2&1\\0&1 \end{pmatrix}=\begin{pmatrix}{6\over 5}&{4\over 5}\\ {4\over 5}&{6\over 5} \end{pmatrix}</script><p>然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为:</p><script type="math/tex; mode=display">\lambda_1=2,\lambda_2={2\over5}</script><p>其对应的特征向量分别是：</p><script type="math/tex; mode=display">C_1=\begin{pmatrix} 1\\1\end{pmatrix},C_2=\begin{pmatrix} -1\\1\end{pmatrix}</script><p>其中对应的特征向量分别是一个通解，$c_1$和可$c_2$取任意实数。那么标准化后的特征向量为：</p><script type="math/tex; mode=display">\begin{pmatrix} {1\over \sqrt{2}}\\{1\over \sqrt{2}}\end{pmatrix},\begin{pmatrix} -{1\over \sqrt{2}}\\{1\over \sqrt{2}}\end{pmatrix}</script><p>因此我们的矩阵P是：</p><script type="math/tex; mode=display">P=\begin{pmatrix} {1\over \sqrt{2}}&{1\over \sqrt{2}}\\-{1\over \sqrt{2}}&{1\over \sqrt{2}}\end{pmatrix}</script><p>可以验证协方差矩阵C的对角化：</p><script type="math/tex; mode=display">PCP^T=\begin{pmatrix} {1\over \sqrt{2}}&{1\over \sqrt{2}}\\-{1\over \sqrt{2}}&{1\over \sqrt{2}}\end{pmatrix}\begin{pmatrix} {6\over 5}&{4\over 5}\\{4 \over 5}&{6\over 5}\end{pmatrix}\begin{pmatrix} {1\over \sqrt{2}}&-{1\over \sqrt{2}}\\{1\over \sqrt{2}}&{1\over \sqrt{2}}\end{pmatrix}=\begin{pmatrix}2&0\\0&{2\over 5}\end{pmatrix}</script><p>最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：</p><script type="math/tex; mode=display">Y=\begin{pmatrix}{1\over \sqrt{2}}&{1\over \sqrt{2}} \end{pmatrix}\begin{pmatrix} -1&-1&0&2&0\\ -2&0&0&1&1\end{pmatrix}=\begin{pmatrix}-{3\over \sqrt{2}}&-{1\over \sqrt{2}}&0&{3\over \sqrt{2}}& -{1\over \sqrt{2}} \end{pmatrix}</script><p><img src="https://pic3.zhimg.com/2988668d28bfefee84b03bbff7dde06f_r.jpg" alt="">降维结果如上图。</p><p><strong>实例2</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">function linear_PCA </span><br><span class="line"></span><br><span class="line">%% PARAMETERS</span><br><span class="line"></span><br><span class="line">N = <span class="number">500</span>;% number of data points</span><br><span class="line">R = [<span class="number">-.9</span> <span class="number">.4</span>; <span class="number">.1</span> <span class="number">.2</span>];% covariance matrix</span><br><span class="line"></span><br><span class="line">%% PROGRAM</span><br><span class="line">tic</span><br><span class="line"></span><br><span class="line">X = randn(N,<span class="number">2</span>)*R;% correlated two-dimensional data</span><br><span class="line"></span><br><span class="line">[E,v,Xp] = km_pca(X,<span class="number">1</span>);% obtain eigenvector matrix E, eigenvalues v <span class="keyword">and</span> principal components Xp</span><br><span class="line"></span><br><span class="line">toc</span><br><span class="line">%% OUTPUT</span><br><span class="line">Y = X*E(:,<span class="number">2</span>);</span><br><span class="line">figure; hold on</span><br><span class="line">plot(X(:,<span class="number">1</span>),X(:,<span class="number">2</span>),<span class="string">'.'</span>)</span><br><span class="line">plot(E(<span class="number">1</span>,<span class="number">1</span>)*Xp,E(<span class="number">2</span>,<span class="number">1</span>)*Xp,<span class="string">'.r'</span>)</span><br><span class="line">plot(E(<span class="number">1</span>,<span class="number">2</span>)*Y,E(<span class="number">2</span>,<span class="number">2</span>)*Y,<span class="string">'.b'</span>)</span><br><span class="line">plot([<span class="number">0</span> E(<span class="number">1</span>,<span class="number">1</span>)],[<span class="number">0</span> E(<span class="number">2</span>,<span class="number">1</span>)],<span class="string">'g'</span>,<span class="string">'LineWidth'</span>,<span class="number">4</span>)</span><br><span class="line">plot([<span class="number">0</span> E(<span class="number">1</span>,<span class="number">2</span>)],[<span class="number">0</span> E(<span class="number">2</span>,<span class="number">2</span>)],<span class="string">'k'</span>,<span class="string">'LineWidth'</span>,<span class="number">4</span>)</span><br><span class="line">axis equal</span><br><span class="line">legend(<span class="string">'data'</span>,<span class="string">'first principal components'</span>,<span class="string">'second principal components'</span>,<span class="string">'first principal direction'</span>,<span class="string">'second principal direction'</span>)</span><br><span class="line">title(<span class="string">'linear PCA demo'</span>)</span><br><span class="line"></span><br><span class="line">function [E,v,Xp] = km_pca(X,m)</span><br><span class="line">N = size(X,<span class="number">1</span>);</span><br><span class="line">[E,V] = eig(X<span class="string">'*X/N);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">v = diag(V);</span></span><br><span class="line"><span class="string">[v,ind] = sort(v,'</span>descend<span class="string">');</span></span><br><span class="line"><span class="string">E = E(:,ind);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Xp = X*E(:,1:m);</span></span><br></pre></td></tr></table></figure><p>结果</p><p><img src="https://pic3.zhimg.com/1908980ff0022446e3a27b78891f0181_r.jpg" alt=""></p><p>(说明，为了画图效果，计算协方差矩阵之前没有将数据中心化)</p><h2 id="12-进一步讨论"><a href="#12-进一步讨论" class="headerlink" title="12. 进一步讨论"></a>12. 进一步讨论</h2><p>根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p><p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p><p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p><p>希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。</p>]]></content>
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
